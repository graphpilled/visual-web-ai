<!DOCTYPE html>
<html>
<head>
  <title>Module API Test</title>
  <style>
    body { font-family: monospace; padding: 20px; background: #1a1a2e; color: #eee; }
    pre { background: #16213e; padding: 15px; border-radius: 8px; overflow-x: auto; }
    .pass { color: #4ade80; }
    .fail { color: #f87171; }
    .epoch { color: #fbbf24; }
    .info { color: #60a5fa; }
  </style>
</head>
<body>
  <h1>Module API Test</h1>
  <p>Testing nn.Sequential, nn.Linear, model.parameters(), and save/load</p>
  <pre id="output"></pre>
  <script type="module">
    import { 
      Tensor, nn, optim, init, 
      Linear, ReLU, Tanh, Sigmoid, Sequential, Softmax,
      clipGradNorm, modelSummary, saveModel
    } from '../src/nn.js';
    
    const log = (msg, cls = null) => {
      const span = cls ? `<span class="${cls}">${msg}</span>` : msg;
      document.getElementById('output').innerHTML += span + '\n';
      console.log(msg);
    };
    
    async function testModuleBasics() {
      log('=== Test 1: Module Basics ===\n');
      
      // Create a linear layer
      const linear = new Linear(4, 3);
      log(`Created Linear(4, 3)`);
      log(`  weight shape: [${linear.weight.shape}]`);
      log(`  bias shape: [${linear.bias.shape}]`);
      
      const params = linear.parameters();
      log(`  parameters(): ${params.length} tensors`);
      log(`  numParameters(): ${linear.numParameters()}`, 'info');
      
      // Forward pass
      const x = Tensor.rand([2, 4]);
      const y = await linear.forward(x);
      log(`\nForward: [2, 4] -> [${y.shape}]`);
      
      const pass = y.shape[0] === 2 && y.shape[1] === 3 && params.length === 2;
      log(`\n${pass ? 'PASS âœ“' : 'FAIL âœ—'}`, pass ? 'pass' : 'fail');
      
      return pass;
    }
    
    async function testSequential() {
      log('\n\n=== Test 2: Sequential Model ===\n');
      
      // Build a model using Sequential
      const model = new Sequential(
        new Linear(2, 4),
        new Tanh(),
        new Linear(4, 1),
        new Sigmoid()
      );
      
      log('Model architecture:');
      log('  Linear(2, 4) -> Tanh -> Linear(4, 1) -> Sigmoid');
      log(`\nTotal parameters: ${model.numParameters()}`, 'info');
      
      // List all parameters
      const params = model.parameters();
      log(`\nParameters (${params.length} tensors):`);
      for (const p of params) {
        log(`  ${p.name}: [${p.shape}] = ${p.size}`);
      }
      
      // Forward pass
      const x = Tensor.from([[0, 0], [0, 1], [1, 0], [1, 1]], [4, 2]);
      const y = await model.forward(x);
      log(`\nForward: [4, 2] -> [${y.shape}]`);
      log(`  Output: [${y.toArray().map(v => v.toFixed(3))}]`);
      
      const pass = y.shape[0] === 4 && y.shape[1] === 1 && params.length === 4;
      log(`\n${pass ? 'PASS âœ“' : 'FAIL âœ—'}`, pass ? 'pass' : 'fail');
      
      return pass;
    }
    
    async function testTrainingWithModules() {
      log('\n\n=== Test 3: Training XOR with Sequential ===\n');
      
      // Build model
      const model = new Sequential(
        new Linear(2, 8),
        new Tanh(),
        new Linear(8, 1),
        new Sigmoid()
      );
      
      log(`Model: 2 -> 8 (tanh) -> 1 (sigmoid)`);
      log(`Parameters: ${model.numParameters()}`, 'info');
      
      // Get parameters and create optimizer
      const optimizer = new optim.Adam(model.parameters(), 0.1);
      
      // XOR data
      const X = [
        Tensor.from([0, 0], [1, 2]),
        Tensor.from([0, 1], [1, 2]),
        Tensor.from([1, 0], [1, 2]),
        Tensor.from([1, 1], [1, 2]),
      ];
      const Y = [
        Tensor.from([0], [1, 1]),
        Tensor.from([1], [1, 1]),
        Tensor.from([1], [1, 1]),
        Tensor.from([0], [1, 1]),
      ];
      
      const epochs = 500;
      
      for (let epoch = 0; epoch < epochs; epoch++) {
        let totalLoss = 0;
        
        for (let i = 0; i < 4; i++) {
          optimizer.zeroGrad();
          
          // Forward through model
          const pred = await model.forward(X[i]);
          
          // MSE loss
          const loss = await nn.mseLoss(pred, Y[i]);
          totalLoss += loss.item();
          
          // Backward
          await loss.backward();
          
          // Gradient clipping
          clipGradNorm(model.parameters(), 1.0);
          
          // Update
          await optimizer.step();
        }
        
        if (epoch % 100 === 0 || epoch === epochs - 1) {
          log(`  Epoch ${epoch}: avg_loss=${(totalLoss / 4).toFixed(4)}`, 'epoch');
        }
      }
      
      // Test
      log('\nTesting:');
      let correct = 0;
      
      model.eval(); // Set to evaluation mode
      
      for (let i = 0; i < 4; i++) {
        const pred = await model.forward(X[i]);
        const predVal = pred._data[0];
        const predClass = predVal > 0.5 ? 1 : 0;
        const target = Y[i]._data[0];
        
        if (predClass === target) correct++;
        log(`  [${X[i]._data}] -> ${predVal.toFixed(4)} (pred: ${predClass}, target: ${target}) ${predClass === target ? 'âœ“' : 'âœ—'}`, predClass === target ? 'pass' : 'fail');
      }
      
      const accuracy = correct / 4;
      log(`\nAccuracy: ${correct}/4 = ${(accuracy * 100).toFixed(0)}%`);
      
      const pass = accuracy >= 0.75;
      log(`${pass ? 'PASS âœ“' : 'FAIL âœ—'}`, pass ? 'pass' : 'fail');
      
      return pass;
    }
    
    async function testGradientClipping() {
      log('\n\n=== Test 4: Gradient Clipping ===\n');
      
      // Create a parameter with large gradients
      const p1 = Tensor.from([1, 2, 3], [3], { requiresGrad: true, name: 'p1' });
      const p2 = Tensor.from([4, 5], [2], { requiresGrad: true, name: 'p2' });
      
      // Simulate large gradients
      p1._grad = new Float32Array([10, 20, 30]); // norm = sqrt(100+400+900) = sqrt(1400) â‰ˆ 37.4
      p2._grad = new Float32Array([40, 50]); // adds 1600+2500 = 4100, total = 5500, norm â‰ˆ 74.2
      
      const normBefore = Math.sqrt(
        p1._grad.reduce((s, g) => s + g * g, 0) +
        p2._grad.reduce((s, g) => s + g * g, 0)
      );
      log(`Gradient norm before clipping: ${normBefore.toFixed(2)}`);
      
      // Clip to max norm of 10
      const normAfter = clipGradNorm([p1, p2], 10.0);
      
      const actualNormAfter = Math.sqrt(
        p1._grad.reduce((s, g) => s + g * g, 0) +
        p2._grad.reduce((s, g) => s + g * g, 0)
      );
      
      log(`Gradient norm after clipping (max=10): ${actualNormAfter.toFixed(2)}`);
      log(`  p1.grad: [${Array.from(p1._grad).map(g => g.toFixed(3))}]`);
      log(`  p2.grad: [${Array.from(p2._grad).map(g => g.toFixed(3))}]`);
      
      const pass = Math.abs(actualNormAfter - 10.0) < 0.1;
      log(`\n${pass ? 'PASS âœ“' : 'FAIL âœ—'} (norm clipped to ~10)`, pass ? 'pass' : 'fail');
      
      return pass;
    }
    
    async function testSaveLoad() {
      log('\n\n=== Test 5: Save/Load State ===\n');
      
      // Create and train a simple model
      const model = new Sequential(
        new Linear(2, 3),
        new ReLU(),
        new Linear(3, 1)
      );
      
      // Modify weights to known values
      model.layers[0].weight._data = new Float32Array([1, 2, 3, 4, 5, 6]);
      model.layers[0].bias._data = new Float32Array([0.1, 0.2, 0.3]);
      
      log('Original model weights:');
      log(`  layer0.weight: [${model.layers[0].weight.toArray().slice(0, 3)}...]`);
      log(`  layer0.bias: [${model.layers[0].bias.toArray()}]`);
      
      // Save state
      const state = model.stateDict();
      log(`\nSaved state dict with ${Object.keys(state).length} entries`);
      
      // Create new model and load state
      const model2 = new Sequential(
        new Linear(2, 3),
        new ReLU(),
        new Linear(3, 1)
      );
      
      log('\nNew model weights (before load):');
      log(`  layer0.weight: [${model2.layers[0].weight.toArray().slice(0, 3).map(v => v.toFixed(3))}...]`);
      
      model2.loadStateDict(state);
      
      log('\nNew model weights (after load):');
      log(`  layer0.weight: [${model2.layers[0].weight.toArray().slice(0, 3)}...]`);
      log(`  layer0.bias: [${model2.layers[0].bias.toArray()}]`);
      
      // Verify
      const match = model.layers[0].weight._data.every((v, i) => 
        Math.abs(v - model2.layers[0].weight._data[i]) < 0.0001
      );
      
      log(`\n${match ? 'PASS âœ“' : 'FAIL âœ—'} (weights match after load)`, match ? 'pass' : 'fail');
      
      return match;
    }
    
    async function testModelSummary() {
      log('\n\n=== Test 6: Model Summary ===\n');
      
      const model = new Sequential(
        new Linear(784, 256),
        new ReLU(),
        new Linear(256, 128),
        new ReLU(),
        new Linear(128, 10),
        new Softmax()
      );
      
      log('MNIST-style classifier:');
      log(`  Input: 784 (28x28 image)`);
      log(`  Hidden: 256 -> 128`);
      log(`  Output: 10 classes\n`);
      
      const summary = modelSummary(model);
      log(summary, 'info');
      
      const expectedParams = 784*256 + 256 + 256*128 + 128 + 128*10 + 10;
      const actualParams = model.numParameters();
      
      log(`Expected: ${expectedParams}, Actual: ${actualParams}`);
      
      const pass = actualParams === expectedParams;
      log(`\n${pass ? 'PASS âœ“' : 'FAIL âœ—'}`, pass ? 'pass' : 'fail');
      
      return pass;
    }
    
    async function runAllTests() {
      log('=== Module API Tests ===\n');
      
      try {
        await init();
        log('âœ“ Runtime initialized\n');
        
        const test1 = await testModuleBasics();
        const test2 = await testSequential();
        const test3 = await testTrainingWithModules();
        const test4 = await testGradientClipping();
        const test5 = await testSaveLoad();
        const test6 = await testModelSummary();
        
        log('\n\n=== Summary ===');
        log(`Test 1 (Module Basics): ${test1 ? 'PASS âœ“' : 'FAIL âœ—'}`, test1 ? 'pass' : 'fail');
        log(`Test 2 (Sequential): ${test2 ? 'PASS âœ“' : 'FAIL âœ—'}`, test2 ? 'pass' : 'fail');
        log(`Test 3 (XOR Training): ${test3 ? 'PASS âœ“' : 'FAIL âœ—'}`, test3 ? 'pass' : 'fail');
        log(`Test 4 (Grad Clipping): ${test4 ? 'PASS âœ“' : 'FAIL âœ—'}`, test4 ? 'pass' : 'fail');
        log(`Test 5 (Save/Load): ${test5 ? 'PASS âœ“' : 'FAIL âœ—'}`, test5 ? 'pass' : 'fail');
        log(`Test 6 (Model Summary): ${test6 ? 'PASS âœ“' : 'FAIL âœ—'}`, test6 ? 'pass' : 'fail');
        
        const allPass = test1 && test2 && test3 && test4 && test5 && test6;
        log(`\n${allPass ? 'ALL TESTS PASSED âœ“' : 'SOME TESTS FAILED âœ—'}`, allPass ? 'pass' : 'fail');
        
        if (allPass) {
          log('\nðŸŽ‰ Full PyTorch-like API is working!');
          log('   - nn.Linear, nn.Sequential');
          log('   - model.parameters()');
          log('   - clipGradNorm()');
          log('   - stateDict() / loadStateDict()');
        }
        
      } catch (e) {
        log(`\nError: ${e.message}`, 'fail');
        console.error(e);
      }
    }
    
    runAllTests();
  </script>
</body>
</html>
