<!DOCTYPE html>
<html>
<head>
  <title>MLP Training Test</title>
  <style>
    body { font-family: monospace; padding: 20px; background: #1a1a2e; color: #eee; }
    pre { background: #16213e; padding: 15px; border-radius: 8px; overflow-x: auto; }
    .pass { color: #4ade80; }
    .fail { color: #f87171; }
    .loss { color: #60a5fa; }
    .epoch { color: #fbbf24; }
  </style>
</head>
<body>
  <h1>MLP Training Test - XOR Problem</h1>
  <pre id="output"></pre>
  <script type="module">
    import { GPURuntime } from '../src/runtime.js';
    import { Autograd } from '../dist/bundle.js';
    
    const log = (msg, cls = null) => {
      const span = cls ? `<span class="${cls}">${msg}</span>` : msg;
      document.getElementById('output').innerHTML += span + '\n';
      console.log(msg);
    };

    // Helper to create a buffer with initial data
    function createParamBuffer(runtime, data) {
      const size = data.length * 4;
      return runtime.createBuffer(size, 
        GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST | GPUBufferUsage.COPY_SRC, 
        data);
    }
    
    // Xavier initialization
    function xavier(fanIn, fanOut) {
      const std = Math.sqrt(2.0 / (fanIn + fanOut));
      return Array.from({ length: fanIn * fanOut }, () => (Math.random() - 0.5) * 2 * std);
    }
    
    async function trainXOR() {
      log('=== MLP Training: XOR Problem ===\n');
      log('Network: 2 -> 4 (tanh) -> 1 (sigmoid)');
      log('Goal: Learn XOR function\n');
      
      const runtime = new GPURuntime();
      await runtime.init();
      log(' Runtime initialized\n');
      
      // XOR training data
      const X = [
        [0, 0],
        [0, 1],
        [1, 0],
        [1, 1]
      ];
      const Y = [0, 1, 1, 0]; // XOR outputs
      
      log('Training data (XOR):');
      X.forEach((x, i) => log(`  [${x}] -> ${Y[i]}`));
      log('');
      
      // Network parameters
      const hiddenSize = 4;
      
      // Initialize weights with Xavier
      let W1 = new Float32Array(xavier(2, hiddenSize)); // [2, 4] -> 8 params
      let b1 = new Float32Array(hiddenSize).fill(0);
      let W2 = new Float32Array(xavier(hiddenSize, 1)); // [4, 1] -> 4 params  
      let b2 = new Float32Array([0]);
      
      log(`Parameters: ${W1.length + b1.length + W2.length + b2.length} total`);
      log(`  W1: [2, ${hiddenSize}] = ${W1.length}`);
      log(`  b1: [${hiddenSize}]`);
      log(`  W2: [${hiddenSize}, 1] = ${W2.length}`);
      log(`  b2: [1]\n`);
      
      // Training hyperparameters
      const lr = 0.5;
      const epochs = 500;
      
      log(`Hyperparameters: lr=${lr}, epochs=${epochs}\n`);
      log('Training...\n');
      
      // Training loop (manual forward/backward for clarity)
      for (let epoch = 0; epoch < epochs; epoch++) {
        let totalLoss = 0;
        
        // Gradient accumulators
        let dW1 = new Float32Array(W1.length).fill(0);
        let db1 = new Float32Array(b1.length).fill(0);
        let dW2 = new Float32Array(W2.length).fill(0);
        let db2 = new Float32Array(b2.length).fill(0);
        
        for (let i = 0; i < X.length; i++) {
          const x = X[i];
          const y = Y[i];
          
          // Forward pass
          // Layer 1: h = tanh(x @ W1 + b1)
          const z1 = new Float32Array(hiddenSize);
          for (let j = 0; j < hiddenSize; j++) {
            z1[j] = x[0] * W1[j] + x[1] * W1[hiddenSize + j] + b1[j];
          }
          const h = z1.map(z => Math.tanh(z));
          
          // Layer 2: out = sigmoid(h @ W2 + b2)
          let z2 = b2[0];
          for (let j = 0; j < hiddenSize; j++) {
            z2 += h[j] * W2[j];
          }
          const out = 1 / (1 + Math.exp(-z2));
          
          // Loss: BCE = -y*log(out) - (1-y)*log(1-out)
          const eps = 1e-7;
          const loss = -y * Math.log(out + eps) - (1 - y) * Math.log(1 - out + eps);
          totalLoss += loss;
          
          // Backward pass
          // d(BCE)/d(out) = -y/out + (1-y)/(1-out)
          // d(sigmoid)/d(z2) = out * (1 - out)
          // d(loss)/d(z2) = (out - y)
          const dz2 = out - y;
          
          // Gradients for W2, b2
          for (let j = 0; j < hiddenSize; j++) {
            dW2[j] += dz2 * h[j];
          }
          db2[0] += dz2;
          
          // Backprop through layer 1
          // dh = dz2 * W2
          // dz1 = dh * (1 - tanh^2)
          const dh = W2.map(w => dz2 * w);
          const dz1 = dh.map((d, j) => d * (1 - h[j] * h[j]));
          
          // Gradients for W1, b1
          for (let j = 0; j < hiddenSize; j++) {
            dW1[j] += dz1[j] * x[0];
            dW1[hiddenSize + j] += dz1[j] * x[1];
            db1[j] += dz1[j];
          }
        }
        
        // Average gradients
        const n = X.length;
        dW1 = dW1.map(g => g / n);
        db1 = db1.map(g => g / n);
        dW2 = dW2.map(g => g / n);
        db2 = db2.map(g => g / n);
        
        // Update parameters (SGD)
        W1 = W1.map((w, i) => w - lr * dW1[i]);
        b1 = b1.map((b, i) => b - lr * db1[i]);
        W2 = W2.map((w, i) => w - lr * dW2[i]);
        b2 = b2.map((b, i) => b - lr * db2[i]);
        
        if (epoch % 100 === 0 || epoch === epochs - 1) {
          log(`  Epoch ${epoch}: loss=${(totalLoss / n).toFixed(4)}`, 'epoch');
        }
      }
      
      // Test the trained network
      log('\nTesting trained network:\n');
      let correct = 0;
      
      for (let i = 0; i < X.length; i++) {
        const x = X[i];
        const y = Y[i];
        
        // Forward pass
        const z1 = new Float32Array(hiddenSize);
        for (let j = 0; j < hiddenSize; j++) {
          z1[j] = x[0] * W1[j] + x[1] * W1[hiddenSize + j] + b1[j];
        }
        const h = z1.map(z => Math.tanh(z));
        
        let z2 = b2[0];
        for (let j = 0; j < hiddenSize; j++) {
          z2 += h[j] * W2[j];
        }
        const out = 1 / (1 + Math.exp(-z2));
        const pred = out > 0.5 ? 1 : 0;
        
        const isCorrect = pred === y;
        if (isCorrect) correct++;
        
        log(`  [${x}] -> ${out.toFixed(4)} (pred: ${pred}, target: ${y}) ${isCorrect ? '' : ''}`, isCorrect ? 'pass' : 'fail');
      }
      
      log(`\nAccuracy: ${correct}/${X.length} = ${(100 * correct / X.length).toFixed(0)}%`);
      log(correct === X.length ? '\nPASS  - XOR learned successfully!' : '\nFAIL ', correct === X.length ? 'pass' : 'fail');
      
      // Now verify GPU backward kernels match CPU
      log('\n\n=== GPU Backward Kernel Verification ===\n');
      
      // Test Tanh backward
      log('Test: Tanh backward (d/dx tanh = 1 - tanh)');
      const tanhKernel = Autograd.genTanhBackwardKernel(4);
      const tanhOut = new Float32Array([0, 0.5, 0.76, -0.76]);
      const tanhGradOut = new Float32Array([1, 1, 1, 1]);
      
      const tanhResult = await runtime.execute(tanhKernel, { workgroupCount: [1, 1, 1] }, {
        grad_out: tanhGradOut,
        out: tanhOut
      });
      
      const expectedTanhGrad = tanhOut.map(t => 1 - t * t);
      log(`  tanh_out: [${Array.from(tanhOut).map(x => x.toFixed(2))}]`);
      log(`  GPU grad: [${Array.from(tanhResult).map(x => x.toFixed(4))}]`);
      log(`  Expected: [${Array.from(expectedTanhGrad).map(x => x.toFixed(4))}]`);
      
      const tanhPass = Array.from(tanhResult).every((v, i) => Math.abs(v - expectedTanhGrad[i]) < 0.01);
      log(`  ${tanhPass ? 'PASS ' : 'FAIL '}`, tanhPass ? 'pass' : 'fail');
      
      // Test Sigmoid backward
      log('\nTest: Sigmoid backward (d/dx sigmoid = sigmoid * (1 - sigmoid))');
      const sigKernel = Autograd.genSigmoidBackwardKernel(4);
      const sigOut = new Float32Array([0.5, 0.73, 0.27, 0.88]);
      const sigGradOut = new Float32Array([1, 1, 1, 1]);
      
      const sigResult = await runtime.execute(sigKernel, { workgroupCount: [1, 1, 1] }, {
        grad_out: sigGradOut,
        out: sigOut
      });
      
      const expectedSigGrad = sigOut.map(s => s * (1 - s));
      log(`  sigmoid_out: [${Array.from(sigOut).map(x => x.toFixed(2))}]`);
      log(`  GPU grad: [${Array.from(sigResult).map(x => x.toFixed(4))}]`);
      log(`  Expected: [${Array.from(expectedSigGrad).map(x => x.toFixed(4))}]`);
      
      const sigPass = Array.from(sigResult).every((v, i) => Math.abs(v - expectedSigGrad[i]) < 0.01);
      log(`  ${sigPass ? 'PASS ' : 'FAIL '}`, sigPass ? 'pass' : 'fail');
      
      // Test MatMul backward
      log('\nTest: MatMul backward');
      // C[2,2] = A[2,3] @ B[3,2]
      // dA = dC @ B^T
      // dB = A^T @ dC
      const matmulBackwardA = Autograd.genMatMulBackwardAKernel(2, 3, 2);
      const matmulBackwardB = Autograd.genMatMulBackwardBKernel(2, 3, 2);
      
      const gradC = new Float32Array([1, 0, 0, 1]); // dL/dC = I
      const A = new Float32Array([1, 2, 3, 4, 5, 6]); // 2x3
      const B = new Float32Array([1, 2, 3, 4, 5, 6]); // 3x2
      
      const gradAResult = await runtime.execute(matmulBackwardA, 
        { workgroupCount: [1, 1, 1] }, 
        { grad_out: gradC, b: B }
      );
      
      const gradBResult = await runtime.execute(matmulBackwardB,
        { workgroupCount: [1, 1, 1] },
        { grad_out: gradC, a: A }
      );
      
      log(`  dC (identity): [[1,0], [0,1]]`);
      log(`  dA = dC @ B^T: [${Array.from(gradAResult).map(x => x.toFixed(1))}]`);
      log(`  dB = A^T @ dC: [${Array.from(gradBResult).map(x => x.toFixed(1))}]`);
      
      // dA = I @ B^T = B^T = [[1,3,5], [2,4,6]]
      const expectedGradA = [1, 3, 5, 2, 4, 6];
      const matmulAPass = Array.from(gradAResult).every((v, i) => Math.abs(v - expectedGradA[i]) < 0.1);
      log(`  Expected dA: [${expectedGradA}]`);
      log(`  ${matmulAPass ? 'PASS ' : 'FAIL '}`, matmulAPass ? 'pass' : 'fail');
      
      runtime.destroy();
      log('\n=== All MLP Training Tests Completed ===');
    }
    
    trainXOR();
  </script>
</body>
</html>
