// Generated by ReScript, PLEASE EDIT WITH CARE

import * as Shape from "./Shape.res.mjs";
import * as Autograd from "./Autograd.res.mjs";
import * as Belt_MapInt from "@rescript/runtime/lib/es6/Belt_MapInt.js";
import * as Belt_SetInt from "@rescript/runtime/lib/es6/Belt_SetInt.js";
import * as Stdlib_Array from "@rescript/runtime/lib/es6/Stdlib_Array.js";
import * as Primitive_int from "@rescript/runtime/lib/es6/Primitive_int.js";
import * as Stdlib_Option from "@rescript/runtime/lib/es6/Stdlib_Option.js";

function create() {
  return {
    ops: [],
    parameterIds: undefined,
    gradients: undefined
  };
}

function markParameter(tape, nodeId) {
  tape.parameterIds = Belt_SetInt.add(tape.parameterIds, nodeId);
}

function isParameter(tape, nodeId) {
  return Belt_SetInt.has(tape.parameterIds, nodeId);
}

function recordOp(tape, nodeId, op, inputIds, inputShapes, outputShape) {
  let record = {
    nodeId: nodeId,
    op: op,
    inputIds: inputIds,
    inputShapes: inputShapes,
    outputShape: outputShape,
    outputId: nodeId
  };
  tape.ops = tape.ops.concat([record]);
}

function reset(tape) {
  tape.ops = [];
  tape.gradients = undefined;
}

function getBackwardKernels(record, _gradOutputShape) {
  let outputSize = Shape.numElements(record.outputShape);
  let match = record.op;
  if (typeof match !== "object") {
    switch (match) {
      case "Identity" :
        return [[
            Autograd.genCopyBackwardKernel(outputSize),
            [
              "grad_out",
              "grad_x"
            ]
          ]];
      case "Neg" :
        return [[
            Autograd.genNegBackwardKernel(outputSize),
            [
              "grad_out",
              "grad_x"
            ]
          ]];
      case "Abs" :
        return [[
            Autograd.genAbsBackwardKernel(outputSize),
            [
              "grad_out",
              "x",
              "grad_x"
            ]
          ]];
      case "Sqrt" :
        return [[
            Autograd.genSqrtBackwardKernel(outputSize),
            [
              "grad_out",
              "out",
              "grad_x"
            ]
          ]];
      case "Exp" :
        return [[
            Autograd.genExpBackwardKernel(outputSize),
            [
              "grad_out",
              "out",
              "grad_x"
            ]
          ]];
      case "Log" :
        return [[
            Autograd.genLogBackwardKernel(outputSize),
            [
              "grad_out",
              "x",
              "grad_x"
            ]
          ]];
      case "Sin" :
        return [[
            Autograd.genSinBackwardKernel(outputSize),
            [
              "grad_out",
              "x",
              "grad_x"
            ]
          ]];
      case "Cos" :
        return [[
            Autograd.genCosBackwardKernel(outputSize),
            [
              "grad_out",
              "x",
              "grad_x"
            ]
          ]];
      case "Tanh" :
        return [[
            Autograd.genTanhBackwardKernel(outputSize),
            [
              "grad_out",
              "out",
              "grad_x"
            ]
          ]];
      case "ReLU" :
        return [[
            Autograd.genReLUBackwardKernel(outputSize),
            [
              "grad_out",
              "x",
              "grad_x"
            ]
          ]];
      case "Sigmoid" :
        return [[
            Autograd.genSigmoidBackwardKernel(outputSize),
            [
              "grad_out",
              "out",
              "grad_x"
            ]
          ]];
      case "GeLU" :
        return [[
            Autograd.genGeLUBackwardKernel(outputSize),
            [
              "grad_out",
              "x",
              "grad_x"
            ]
          ]];
      case "Add" :
        let inputShape0 = Stdlib_Option.getOr(record.inputShapes[0], []);
        let inputShape1 = Stdlib_Option.getOr(record.inputShapes[1], []);
        let kernels = [[
            Autograd.genAddBackwardKernel(outputSize),
            [
              "grad_out",
              "grad_a",
              "grad_b"
            ]
          ]];
        let input0Size = Shape.numElements(inputShape0);
        let input1Size = Shape.numElements(inputShape1);
        if (input0Size < outputSize) {
          let reduceKernel = Autograd.genGradReduceKernel(record.outputShape, inputShape0);
          return kernels.concat([[
              reduceKernel,
              [
                "grad_a_full",
                "grad_a_reduced"
              ]
            ]]);
        }
        if (input1Size >= outputSize) {
          return kernels;
        }
        let reduceKernel$1 = Autograd.genGradReduceKernel(record.outputShape, inputShape1);
        return kernels.concat([[
            reduceKernel$1,
            [
              "grad_b_full",
              "grad_b_reduced"
            ]
          ]]);
      case "Sub" :
        return [[
            Autograd.genSubBackwardKernel(outputSize),
            [
              "grad_out",
              "grad_a",
              "grad_b"
            ]
          ]];
      case "Mul" :
        return [[
            Autograd.genMulBackwardKernel(outputSize),
            [
              "grad_out",
              "a",
              "b",
              "grad_a",
              "grad_b"
            ]
          ]];
      case "Div" :
        return [[
            Autograd.genDivBackwardKernel(outputSize),
            [
              "grad_out",
              "a",
              "b",
              "grad_a",
              "grad_b"
            ]
          ]];
      case "Pow" :
        return [[
            Autograd.genPowBackwardKernel(outputSize),
            [
              "grad_out",
              "a",
              "b",
              "out",
              "grad_a",
              "grad_b"
            ]
          ]];
      case "Maximum" :
        return [[
            Autograd.genMaximumBackwardKernel(outputSize),
            [
              "grad_out",
              "a",
              "b",
              "grad_a",
              "grad_b"
            ]
          ]];
      case "Minimum" :
        return [[
            Autograd.genMinimumBackwardKernel(outputSize),
            [
              "grad_out",
              "a",
              "b",
              "grad_a",
              "grad_b"
            ]
          ]];
      case "MatMul" :
        let shape0 = Stdlib_Option.getOr(record.inputShapes[0], []);
        let shape1 = Stdlib_Option.getOr(record.inputShapes[1], []);
        let r0 = shape0.length;
        let r1 = shape1.length;
        if (!(r0 >= 2 && r1 >= 2)) {
          return [];
        }
        let m = Stdlib_Option.getOr(shape0[r0 - 2 | 0], 1);
        let k = Stdlib_Option.getOr(shape0[r0 - 1 | 0], 1);
        let n = Stdlib_Option.getOr(shape1[r1 - 1 | 0], 1);
        let batchDims = record.outputShape.slice(0, record.outputShape.length - 2 | 0);
        let batchSize = Stdlib_Array.reduce(batchDims, 1, (a, b) => a * b | 0);
        if (batchSize > 1) {
          return [
            [
              Autograd.genBatchedMatMulBackwardAKernel(batchSize, m, k, n),
              [
                "grad_out",
                "b",
                "grad_a"
              ]
            ],
            [
              Autograd.genBatchedMatMulBackwardBKernel(batchSize, m, k, n),
              [
                "grad_out",
                "a",
                "grad_b"
              ]
            ]
          ];
        } else {
          return [
            [
              Autograd.genMatMulBackwardAKernel(m, k, n),
              [
                "grad_out",
                "b",
                "grad_a"
              ]
            ],
            [
              Autograd.genMatMulBackwardBKernel(m, k, n),
              [
                "grad_out",
                "a",
                "grad_b"
              ]
            ]
          ];
        }
      default:
        return [];
    }
  } else {
    switch (match.TAG) {
      case "LeakyReLU" :
        return [[
            Autograd.genLeakyReLUBackwardKernel(outputSize, match.alpha),
            [
              "grad_out",
              "x",
              "grad_x"
            ]
          ]];
      case "Reduce" :
        switch (match.op) {
          case "Sum" :
            let inputShape = Stdlib_Option.getOr(record.inputShapes[0], []);
            return [[
                Autograd.genSumBackwardKernel(inputShape, record.outputShape, match.axes),
                [
                  "grad_out",
                  "grad_x"
                ]
              ]];
          case "Mean" :
            let inputShape$1 = Stdlib_Option.getOr(record.inputShapes[0], []);
            return [[
                Autograd.genMeanBackwardKernel(inputShape$1, record.outputShape, match.axes),
                [
                  "grad_out",
                  "grad_x"
                ]
              ]];
          default:
            return [];
        }
      case "Transpose" :
        return [[
            Autograd.genCopyBackwardKernel(outputSize),
            [
              "grad_out",
              "grad_x"
            ]
          ]];
      case "Reshape" :
      case "Squeeze" :
      case "Unsqueeze" :
      case "Flatten" :
      case "ExpandDims" :
        break;
      case "LayerNorm" :
        let inputShape$2 = Stdlib_Option.getOr(record.inputShapes[0], []);
        let rank = inputShape$2.length;
        let normAxes = match.axes.map(a => {
          if (a < 0) {
            return rank + a | 0;
          } else {
            return a;
          }
        });
        let normSize = Stdlib_Array.reduce(normAxes, 1, (acc, axis) => acc * Stdlib_Option.getOr(inputShape$2[axis], 1) | 0);
        let outerSize = Primitive_int.div(Shape.numElements(inputShape$2), normSize);
        return [[
            Autograd.genLayerNormBackwardKernel(outerSize, normSize, match.epsilon),
            [
              "grad_out",
              "x",
              "gamma",
              "grad_x",
              "grad_gamma",
              "grad_beta"
            ]
          ]];
      case "Softmax" :
        let axis = match.axis;
        let inputShape$3 = Stdlib_Option.getOr(record.inputShapes[0], []);
        let rank$1 = inputShape$3.length;
        let normAxis = axis < 0 ? rank$1 + axis | 0 : axis;
        let axisSize = Stdlib_Option.getOr(inputShape$3[normAxis], 1);
        let outerSize$1 = Primitive_int.div(Shape.numElements(inputShape$3), axisSize);
        return [[
            Autograd.genSoftmaxBackwardKernel(outerSize$1, axisSize),
            [
              "grad_out",
              "softmax_out",
              "grad_x"
            ]
          ]];
      default:
        return [];
    }
  }
  return [[
      Autograd.genCopyBackwardKernel(outputSize),
      [
        "grad_out",
        "grad_x"
      ]
    ]];
}

function supportsGradient(op) {
  if (typeof op !== "object") {
    switch (op) {
      case "Identity" :
      case "Neg" :
      case "Abs" :
      case "Sqrt" :
      case "Exp" :
      case "Log" :
      case "Sin" :
      case "Cos" :
      case "Tanh" :
      case "ReLU" :
      case "Sigmoid" :
      case "GeLU" :
      case "Add" :
      case "Sub" :
      case "Mul" :
      case "Div" :
      case "Pow" :
      case "Maximum" :
      case "Minimum" :
      case "MatMul" :
        return true;
      default:
        return false;
    }
  } else {
    switch (op.TAG) {
      case "Reduce" :
        switch (op.op) {
          case "Sum" :
          case "Mean" :
            return true;
          default:
            return false;
        }
      case "LeakyReLU" :
      case "Reshape" :
      case "Squeeze" :
      case "Unsqueeze" :
      case "Flatten" :
      case "Transpose" :
      case "ExpandDims" :
      case "LayerNorm" :
      case "Softmax" :
        return true;
      default:
        return false;
    }
  }
}

function topoSortBackward(ops) {
  return ops.toReversed();
}

function compileBackward(tape, lossNodeId, nodeShapes) {
  let ops = tape.ops.toReversed();
  let kernels = {
    contents: []
  };
  let dispatches = {
    contents: []
  };
  let buffers = {
    contents: []
  };
  let nextBufferId = {
    contents: 0
  };
  let gradientBufferIds = {
    contents: undefined
  };
  let paramGradIds = {
    contents: []
  };
  let lossShape = Stdlib_Option.getOr(Belt_MapInt.get(nodeShapes, lossNodeId), [1]);
  let lossGradSize = Shape.numElements(lossShape);
  let lossGradBufferId = nextBufferId.contents;
  nextBufferId.contents = nextBufferId.contents + 1 | 0;
  buffers.contents = buffers.contents.concat([{
      id: lossGradBufferId,
      size: (lossGradSize << 2),
      name: "grad_loss"
    }]);
  gradientBufferIds.contents = Belt_MapInt.set(gradientBufferIds.contents, lossNodeId, lossGradBufferId);
  ops.forEach(record => {
    if (!supportsGradient(record.op)) {
      return;
    }
    let gradOutShape = Stdlib_Option.getOr(Belt_MapInt.get(nodeShapes, record.outputId), []);
    let backwardKernels = getBackwardKernels(record, gradOutShape);
    backwardKernels.forEach(param => {
      let kernel = param[0];
      record.inputIds.forEach((inputId, _idx) => {
        if (Belt_MapInt.has(gradientBufferIds.contents, inputId)) {
          return;
        }
        let inputShape = Stdlib_Option.getOr(record.inputShapes[_idx], []);
        let gradSize = Shape.numElements(inputShape);
        let gradBufferId = nextBufferId.contents;
        nextBufferId.contents = nextBufferId.contents + 1 | 0;
        buffers.contents = buffers.contents.concat([{
            id: gradBufferId,
            size: (gradSize << 2),
            name: "grad_" + inputId.toString()
          }]);
        gradientBufferIds.contents = Belt_MapInt.set(gradientBufferIds.contents, inputId, gradBufferId);
        if (Belt_SetInt.has(tape.parameterIds, inputId)) {
          paramGradIds.contents = paramGradIds.contents.concat([[
              inputId,
              gradBufferId
            ]]);
          return;
        }
      });
      kernels.contents = kernels.contents.concat([kernel]);
      let totalElements = Shape.numElements(gradOutShape);
      let workgroupCount = (totalElements + 255 | 0) / 256 | 0;
      dispatches.contents = dispatches.contents.concat([{
          workgroupSize: [
            256,
            1,
            1
          ],
          workgroupCount: [
            workgroupCount,
            1,
            1
          ],
          kernelName: kernel.name,
          pipelineIndex: kernels.contents.length - 1 | 0
        }]);
    });
  });
  return {
    kernels: kernels.contents,
    dispatches: dispatches.contents,
    buffers: buffers.contents,
    gradientBufferIds: gradientBufferIds.contents,
    parameterGradientIds: paramGradIds.contents
  };
}

export {
  create,
  markParameter,
  isParameter,
  recordOp,
  reset,
  getBackwardKernels,
  supportsGradient,
  topoSortBackward,
  compileBackward,
}
/* Autograd Not a pure module */
