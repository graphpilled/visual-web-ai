<!DOCTYPE html>
<html>
<head>
  <title>End-to-End Autograd Test</title>
  <style>
    body { font-family: monospace; padding: 20px; background: #1a1a2e; color: #eee; }
    pre { background: #16213e; padding: 15px; border-radius: 8px; overflow-x: auto; }
    .pass { color: #4ade80; }
    .fail { color: #f87171; }
    .loss { color: #60a5fa; }
    .epoch { color: #fbbf24; }
    .grad { color: #c084fc; }
  </style>
</head>
<body>
  <h1>End-to-End Autograd Test</h1>
  <p>Testing that loss.backward() automatically computes gradients on the GPU</p>
  <pre id="output"></pre>
  <script type="module">
    import { Tensor, init, mseLoss, clearTape, nn, optim } from '../src/nn.js';
    
    const log = (msg, cls = null) => {
      const span = cls ? `<span class="${cls}">${msg}</span>` : msg;
      document.getElementById('output').innerHTML += span + '\n';
      console.log(msg);
    };
    
    async function testBasicBackward() {
      log('=== Test 1: Basic Backward (y = x * 2) ===\n');
      
      const x = Tensor.from([1, 2, 3, 4], [4], { requiresGrad: true, name: 'x' });
      log(`x = [${x.toArray()}]  (requiresGrad: true)`);
      
      const two = Tensor.from([2], [1]);
      const y = await x.mul(two);
      log(`y = x * 2 = [${y.toArray()}]`);
      
      const loss = await y.mean();
      log(`loss = mean(y) = ${loss.item()}`);
      
      log('\nCalling loss.backward()...');
      await loss.backward();
      
      // d(loss)/d(x) = d(mean(x*2))/d(x) = 2/n = 2/4 = 0.5
      const expectedGrad = [0.5, 0.5, 0.5, 0.5];
      log(`\nx.grad = [${x.grad().map(g => g.toFixed(4))}]`, 'grad');
      log(`Expected: [${expectedGrad}]`);
      
      const pass = expectedGrad.every((v, i) => Math.abs(x.grad()[i] - v) < 0.01);
      log(`\n${pass ? 'PASS âœ“' : 'FAIL âœ—'}`, pass ? 'pass' : 'fail');
      
      clearTape();
      return pass;
    }
    
    async function testChainedOps() {
      log('\n\n=== Test 2: Chained Operations ===\n');
      
      const x = Tensor.from([1, 2, 3, 4], [4], { requiresGrad: true, name: 'x' });
      log(`x = [${x.toArray()}]`);
      
      // y = relu(x - 2)
      const two = Tensor.from([2], [1]);
      const shifted = await x.sub(two);
      log(`x - 2 = [${shifted.toArray()}]`);
      
      const y = await shifted.relu();
      log(`relu(x - 2) = [${y.toArray()}]`);
      
      const loss = await y.sum();
      log(`loss = sum(y) = ${loss.item()}`);
      
      log('\nCalling loss.backward()...');
      await loss.backward();
      
      // d(loss)/d(x) = d(sum(relu(x-2)))/d(x)
      // relu'(z) = 1 if z > 0, else 0
      // x - 2 = [-1, 0, 1, 2], relu derivative = [0, 0, 1, 1]
      const expectedGrad = [0, 0, 1, 1];
      log(`\nx.grad = [${x.grad().map(g => g.toFixed(4))}]`, 'grad');
      log(`Expected: [${expectedGrad}]`);
      
      const pass = expectedGrad.every((v, i) => Math.abs(x.grad()[i] - v) < 0.01);
      log(`\n${pass ? 'PASS âœ“' : 'FAIL âœ—'}`, pass ? 'pass' : 'fail');
      
      clearTape();
      return pass;
    }
    
    async function testMatMulBackward() {
      log('\n\n=== Test 3: MatMul Backward ===\n');
      
      // y = x @ W, where x: [2, 3], W: [3, 2]
      const x = Tensor.from([1, 2, 3, 4, 5, 6], [2, 3], { requiresGrad: true, name: 'x' });
      const W = Tensor.from([0.1, 0.2, 0.3, 0.4, 0.5, 0.6], [3, 2], { requiresGrad: true, name: 'W' });
      
      log(`x = [[1,2,3], [4,5,6]]  shape: [2, 3]`);
      log(`W = [[0.1,0.2], [0.3,0.4], [0.5,0.6]]  shape: [3, 2]`);
      
      const y = await x.matmul(W);
      log(`\ny = x @ W = [${y.toArray().map(v => v.toFixed(2))}]  shape: [2, 2]`);
      
      const loss = await y.mean();
      log(`loss = mean(y) = ${loss.item().toFixed(4)}`);
      
      log('\nCalling loss.backward()...');
      await loss.backward();
      
      log(`\nx.grad shape: [${x.grad().length}]`, 'grad');
      log(`x.grad = [${x.grad().map(g => g.toFixed(4))}]`, 'grad');
      log(`W.grad shape: [${W.grad().length}]`, 'grad');
      log(`W.grad = [${W.grad().map(g => g.toFixed(4))}]`, 'grad');
      
      // Verify gradients are non-zero and reasonable
      const xGradNorm = Math.sqrt(x.grad().reduce((s, g) => s + g * g, 0));
      const wGradNorm = Math.sqrt(W.grad().reduce((s, g) => s + g * g, 0));
      
      log(`\n|x.grad| = ${xGradNorm.toFixed(4)}`);
      log(`|W.grad| = ${wGradNorm.toFixed(4)}`);
      
      const pass = xGradNorm > 0.01 && wGradNorm > 0.01;
      log(`\n${pass ? 'PASS âœ“ (gradients computed)' : 'FAIL âœ—'}`, pass ? 'pass' : 'fail');
      
      clearTape();
      return pass;
    }
    
    async function testLinearRegressionWithAutograd() {
      log('\n\n=== Test 4: Linear Regression with Automatic Backward ===\n');
      log('Learning y = 2*x + 1 using loss.backward()\n');
      
      // Training data
      const X = Tensor.from([[0], [1], [2], [3]], [4, 1]);
      const Y = Tensor.from([[1], [3], [5], [7]], [4, 1]);
      
      // Parameters
      const W = Tensor.from([[0.5]], [1, 1], { requiresGrad: true, name: 'W' });
      const b = Tensor.from([0.0], [1], { requiresGrad: true, name: 'b' });
      
      const optimizer = new optim.SGD([W, b], 0.1);
      
      log(`Initial: W = ${W._data[0].toFixed(4)}, b = ${b._data[0].toFixed(4)}`);
      
      const epochs = 100;
      
      for (let epoch = 0; epoch < epochs; epoch++) {
        optimizer.zeroGrad();
        
        // Forward: pred = X @ W + b
        const pred = await X.matmul(W);
        const predWithBias = await pred.add(b);
        
        // Loss: MSE
        const diff = await predWithBias.sub(Y);
        const sq = await diff.mul(diff);
        const loss = await sq.mean();
        
        // Backward - THE KEY PART!
        await loss.backward();
        
        // Optimizer step
        await optimizer.step();
        
        if (epoch % 20 === 0 || epoch === epochs - 1) {
          log(`  Epoch ${epoch}: loss=${loss.item().toFixed(4)}, W=${W._data[0].toFixed(4)}, b=${b._data[0].toFixed(4)}`, 'epoch');
        }
      }
      
      const wPass = Math.abs(W._data[0] - 2.0) < 0.15;
      const bPass = Math.abs(b._data[0] - 1.0) < 0.15;
      
      log(`\nFinal: W = ${W._data[0].toFixed(4)} (target: 2.0), b = ${b._data[0].toFixed(4)} (target: 1.0)`);
      log(`${wPass && bPass ? 'PASS âœ“' : 'FAIL âœ—'}`, wPass && bPass ? 'pass' : 'fail');
      
      return wPass && bPass;
    }
    
    async function testXORWithAutograd() {
      log('\n\n=== Test 5: XOR with Automatic Backward + Adam ===\n');
      log('Network: 2 -> 4 (tanh) -> 1 (sigmoid)\n');
      
      // XOR data
      const X = [
        Tensor.from([0, 0], [1, 2]),
        Tensor.from([0, 1], [1, 2]),
        Tensor.from([1, 0], [1, 2]),
        Tensor.from([1, 1], [1, 2]),
      ];
      const Y = [
        Tensor.from([0], [1, 1]),
        Tensor.from([1], [1, 1]),
        Tensor.from([1], [1, 1]),
        Tensor.from([0], [1, 1]),
      ];
      
      // Parameters
      const W1 = Tensor.xavier([2, 4], { requiresGrad: true, name: 'W1' });
      const b1 = Tensor.zeros([4], { requiresGrad: true, name: 'b1' });
      const W2 = Tensor.xavier([4, 1], { requiresGrad: true, name: 'W2' });
      const b2 = Tensor.zeros([1], { requiresGrad: true, name: 'b2' });
      
      const optimizer = new optim.Adam([W1, b1, W2, b2], 0.1);
      
      const epochs = 500;
      
      for (let epoch = 0; epoch < epochs; epoch++) {
        let totalLoss = 0;
        
        for (let i = 0; i < 4; i++) {
          optimizer.zeroGrad();
          
          const x = X[i];
          const y = Y[i];
          
          // Forward: h = tanh(x @ W1 + b1)
          const z1 = await x.matmul(W1);
          const z1b = await z1.add(b1);
          const h = await z1b.tanh();
          
          // out = sigmoid(h @ W2 + b2)
          const z2 = await h.matmul(W2);
          const z2b = await z2.add(b2);
          const out = await z2b.sigmoid();
          
          // BCE loss (simplified)
          const diff = await out.sub(y);
          const sq = await diff.mul(diff);
          const loss = await sq.mean();
          
          totalLoss += loss.item();
          
          // Backward with automatic gradient computation!
          await loss.backward();
          
          // Update
          await optimizer.step();
        }
        
        if (epoch % 100 === 0 || epoch === epochs - 1) {
          log(`  Epoch ${epoch}: avg_loss=${(totalLoss / 4).toFixed(4)}`, 'epoch');
        }
      }
      
      // Test
      log('\nTesting:');
      let correct = 0;
      
      for (let i = 0; i < 4; i++) {
        const x = X[i];
        const target = Y[i]._data[0];
        
        // Forward
        const z1 = await x.matmul(W1);
        const z1b = await z1.add(b1);
        const h = await z1b.tanh();
        const z2 = await h.matmul(W2);
        const z2b = await z2.add(b2);
        const out = await z2b.sigmoid();
        
        const pred = out._data[0] > 0.5 ? 1 : 0;
        if (pred === target) correct++;
        
        log(`  [${x._data}] -> ${out._data[0].toFixed(4)} (pred: ${pred}, target: ${target}) ${pred === target ? 'âœ“' : 'âœ—'}`, pred === target ? 'pass' : 'fail');
      }
      
      const accuracy = correct / 4;
      log(`\nAccuracy: ${correct}/4 = ${(accuracy * 100).toFixed(0)}%`);
      log(`${accuracy >= 0.75 ? 'PASS âœ“' : 'FAIL âœ—'}`, accuracy >= 0.75 ? 'pass' : 'fail');
      
      return accuracy >= 0.75;
    }
    
    async function runAllTests() {
      log('=== End-to-End Autograd Tests ===');
      log('All gradients computed automatically via loss.backward()\n');
      
      try {
        await init();
        log('âœ“ Runtime initialized\n');
        
        const test1 = await testBasicBackward();
        const test2 = await testChainedOps();
        const test3 = await testMatMulBackward();
        const test4 = await testLinearRegressionWithAutograd();
        const test5 = await testXORWithAutograd();
        
        log('\n\n=== Summary ===');
        log(`Test 1 (Basic mul+mean): ${test1 ? 'PASS âœ“' : 'FAIL âœ—'}`, test1 ? 'pass' : 'fail');
        log(`Test 2 (Chained sub+relu): ${test2 ? 'PASS âœ“' : 'FAIL âœ—'}`, test2 ? 'pass' : 'fail');
        log(`Test 3 (MatMul backward): ${test3 ? 'PASS âœ“' : 'FAIL âœ—'}`, test3 ? 'pass' : 'fail');
        log(`Test 4 (Linear Regression): ${test4 ? 'PASS âœ“' : 'FAIL âœ—'}`, test4 ? 'pass' : 'fail');
        log(`Test 5 (XOR + Adam): ${test5 ? 'PASS âœ“' : 'FAIL âœ—'}`, test5 ? 'pass' : 'fail');
        
        const allPass = test1 && test2 && test3 && test4 && test5;
        log(`\n${allPass ? 'ALL TESTS PASSED âœ“' : 'SOME TESTS FAILED âœ—'}`, allPass ? 'pass' : 'fail');
        
        if (allPass) {
          log('\nðŸŽ‰ Automatic differentiation is fully working!');
          log('   - Forward ops recorded to tape');
          log('   - loss.backward() traverses tape in reverse');
          log('   - Gradients computed on GPU via backward kernels');
          log('   - Optimizers update parameters on GPU');
        }
        
      } catch (e) {
        log(`\nError: ${e.message}`, 'fail');
        console.error(e);
      }
    }
    
    runAllTests();
  </script>
</body>
</html>
