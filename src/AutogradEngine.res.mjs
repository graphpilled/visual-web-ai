// Generated by ReScript, PLEASE EDIT WITH CARE

import * as Shape from "./Shape.res.mjs";
import * as Autograd from "./Autograd.res.mjs";
import * as Belt_MapInt from "@rescript/runtime/lib/es6/Belt_MapInt.js";
import * as Belt_SetInt from "@rescript/runtime/lib/es6/Belt_SetInt.js";
import * as Stdlib_Array from "@rescript/runtime/lib/es6/Stdlib_Array.js";
import * as Primitive_int from "@rescript/runtime/lib/es6/Primitive_int.js";
import * as Stdlib_Option from "@rescript/runtime/lib/es6/Stdlib_Option.js";

function create() {
  return {
    tape: [],
    parameters: [],
    requiresGrad: undefined,
    gradients: undefined,
    adamStates: undefined,
    timestep: 0,
    bufferCache: undefined,
    shapeCache: undefined
  };
}

function markRequiresGrad(engine, nodeId, shape, name) {
  engine.requiresGrad = Belt_SetInt.add(engine.requiresGrad, nodeId);
  engine.parameters = engine.parameters.concat([{
      nodeId: nodeId,
      bufferId: -1,
      shape: shape,
      name: name
    }]);
  engine.shapeCache = Belt_MapInt.set(engine.shapeCache, nodeId, shape);
}

function needsGrad(engine, nodeId) {
  return Belt_SetInt.has(engine.requiresGrad, nodeId);
}

function record(engine, nodeId, op, inputNodeIds, inputBufferIds, inputShapes, outputBufferId, outputShape) {
  let anyRequiresGrad = inputNodeIds.some(id => Belt_SetInt.has(engine.requiresGrad, id));
  if (!anyRequiresGrad) {
    return;
  }
  let inputs = inputNodeIds.map((id, idx) => ({
    nodeId: id,
    bufferId: Stdlib_Option.getOr(inputBufferIds[idx], -1),
    shape: Stdlib_Option.getOr(inputShapes[idx], []),
    data: undefined
  }));
  let output = {
    nodeId: nodeId,
    bufferId: outputBufferId,
    shape: outputShape,
    data: undefined
  };
  engine.tape = engine.tape.concat([{
      nodeId: nodeId,
      op: op,
      inputs: inputs,
      output: output
    }]);
  engine.bufferCache = Belt_MapInt.set(engine.bufferCache, nodeId, outputBufferId);
  engine.shapeCache = Belt_MapInt.set(engine.shapeCache, nodeId, outputShape);
  engine.requiresGrad = Belt_SetInt.add(engine.requiresGrad, nodeId);
}

function clearTape(engine) {
  engine.tape = [];
}

function getOrAllocateGradient(engine, nodeId, _size) {
  let bufferId = Belt_MapInt.get(engine.gradients, nodeId);
  if (bufferId !== undefined) {
    return bufferId;
  }
  let bufferId$1 = (nodeId * 1000 | 0) + 500 | 0;
  engine.gradients = Belt_MapInt.set(engine.gradients, nodeId, bufferId$1);
  return bufferId$1;
}

function generateBackwardOps(engine, lossNodeId) {
  let ops = {
    contents: []
  };
  let reversedTape = engine.tape.toReversed();
  let lossShape = Stdlib_Option.getOr(Belt_MapInt.get(engine.shapeCache, lossNodeId), [1]);
  let lossSize = Shape.numElements(lossShape);
  getOrAllocateGradient(engine, lossNodeId, lossSize);
  reversedTape.forEach(record => {
    let outputSize = Shape.numElements(record.output.shape);
    let gradOutBufferId = getOrAllocateGradient(engine, record.nodeId, outputSize);
    let match = record.op;
    if (typeof match !== "object") {
      switch (match) {
        case "Identity" :
          break;
        case "Neg" :
          let inputId = Stdlib_Option.getOr(Stdlib_Option.map(record.inputs[0], i => i.nodeId), -1);
          let inputSize = Stdlib_Option.getOr(Stdlib_Option.map(record.inputs[0], i => Shape.numElements(i.shape)), 0);
          let gradInBufferId = getOrAllocateGradient(engine, inputId, inputSize);
          ops.contents = ops.contents.concat([{
              kernel: Autograd.genNegBackwardKernel(outputSize),
              bindings: [
                [
                  "grad_out",
                  gradOutBufferId
                ],
                [
                  "grad_x",
                  gradInBufferId
                ]
              ],
              outputSize: outputSize
            }]);
          return;
        case "Abs" :
          let input = Stdlib_Option.getOr(record.inputs[0], {
            nodeId: -1,
            bufferId: -1,
            shape: [],
            data: undefined
          });
          let inputSize$1 = Shape.numElements(input.shape);
          let gradInBufferId$1 = getOrAllocateGradient(engine, input.nodeId, inputSize$1);
          ops.contents = ops.contents.concat([{
              kernel: Autograd.genAbsBackwardKernel(outputSize),
              bindings: [
                [
                  "grad_out",
                  gradOutBufferId
                ],
                [
                  "x",
                  input.bufferId
                ],
                [
                  "grad_x",
                  gradInBufferId$1
                ]
              ],
              outputSize: outputSize
            }]);
          return;
        case "Sqrt" :
          let input$1 = Stdlib_Option.getOr(record.inputs[0], {
            nodeId: -1,
            bufferId: -1,
            shape: [],
            data: undefined
          });
          let inputSize$2 = Shape.numElements(input$1.shape);
          let gradInBufferId$2 = getOrAllocateGradient(engine, input$1.nodeId, inputSize$2);
          ops.contents = ops.contents.concat([{
              kernel: Autograd.genSqrtBackwardKernel(outputSize),
              bindings: [
                [
                  "grad_out",
                  gradOutBufferId
                ],
                [
                  "out",
                  record.output.bufferId
                ],
                [
                  "grad_x",
                  gradInBufferId$2
                ]
              ],
              outputSize: outputSize
            }]);
          return;
        case "Exp" :
          let input$2 = Stdlib_Option.getOr(record.inputs[0], {
            nodeId: -1,
            bufferId: -1,
            shape: [],
            data: undefined
          });
          let inputSize$3 = Shape.numElements(input$2.shape);
          let gradInBufferId$3 = getOrAllocateGradient(engine, input$2.nodeId, inputSize$3);
          ops.contents = ops.contents.concat([{
              kernel: Autograd.genExpBackwardKernel(outputSize),
              bindings: [
                [
                  "grad_out",
                  gradOutBufferId
                ],
                [
                  "out",
                  record.output.bufferId
                ],
                [
                  "grad_x",
                  gradInBufferId$3
                ]
              ],
              outputSize: outputSize
            }]);
          return;
        case "Log" :
          let input$3 = Stdlib_Option.getOr(record.inputs[0], {
            nodeId: -1,
            bufferId: -1,
            shape: [],
            data: undefined
          });
          let inputSize$4 = Shape.numElements(input$3.shape);
          let gradInBufferId$4 = getOrAllocateGradient(engine, input$3.nodeId, inputSize$4);
          ops.contents = ops.contents.concat([{
              kernel: Autograd.genLogBackwardKernel(outputSize),
              bindings: [
                [
                  "grad_out",
                  gradOutBufferId
                ],
                [
                  "x",
                  input$3.bufferId
                ],
                [
                  "grad_x",
                  gradInBufferId$4
                ]
              ],
              outputSize: outputSize
            }]);
          return;
        case "Sin" :
          let input$4 = Stdlib_Option.getOr(record.inputs[0], {
            nodeId: -1,
            bufferId: -1,
            shape: [],
            data: undefined
          });
          let inputSize$5 = Shape.numElements(input$4.shape);
          let gradInBufferId$5 = getOrAllocateGradient(engine, input$4.nodeId, inputSize$5);
          ops.contents = ops.contents.concat([{
              kernel: Autograd.genSinBackwardKernel(outputSize),
              bindings: [
                [
                  "grad_out",
                  gradOutBufferId
                ],
                [
                  "x",
                  input$4.bufferId
                ],
                [
                  "grad_x",
                  gradInBufferId$5
                ]
              ],
              outputSize: outputSize
            }]);
          return;
        case "Cos" :
          let input$5 = Stdlib_Option.getOr(record.inputs[0], {
            nodeId: -1,
            bufferId: -1,
            shape: [],
            data: undefined
          });
          let inputSize$6 = Shape.numElements(input$5.shape);
          let gradInBufferId$6 = getOrAllocateGradient(engine, input$5.nodeId, inputSize$6);
          ops.contents = ops.contents.concat([{
              kernel: Autograd.genCosBackwardKernel(outputSize),
              bindings: [
                [
                  "grad_out",
                  gradOutBufferId
                ],
                [
                  "x",
                  input$5.bufferId
                ],
                [
                  "grad_x",
                  gradInBufferId$6
                ]
              ],
              outputSize: outputSize
            }]);
          return;
        case "Tanh" :
          let input$6 = Stdlib_Option.getOr(record.inputs[0], {
            nodeId: -1,
            bufferId: -1,
            shape: [],
            data: undefined
          });
          let inputSize$7 = Shape.numElements(input$6.shape);
          let gradInBufferId$7 = getOrAllocateGradient(engine, input$6.nodeId, inputSize$7);
          ops.contents = ops.contents.concat([{
              kernel: Autograd.genTanhBackwardKernel(outputSize),
              bindings: [
                [
                  "grad_out",
                  gradOutBufferId
                ],
                [
                  "out",
                  record.output.bufferId
                ],
                [
                  "grad_x",
                  gradInBufferId$7
                ]
              ],
              outputSize: outputSize
            }]);
          return;
        case "ReLU" :
          let input$7 = Stdlib_Option.getOr(record.inputs[0], {
            nodeId: -1,
            bufferId: -1,
            shape: [],
            data: undefined
          });
          let inputSize$8 = Shape.numElements(input$7.shape);
          let gradInBufferId$8 = getOrAllocateGradient(engine, input$7.nodeId, inputSize$8);
          ops.contents = ops.contents.concat([{
              kernel: Autograd.genReLUBackwardKernel(outputSize),
              bindings: [
                [
                  "grad_out",
                  gradOutBufferId
                ],
                [
                  "x",
                  input$7.bufferId
                ],
                [
                  "grad_x",
                  gradInBufferId$8
                ]
              ],
              outputSize: outputSize
            }]);
          return;
        case "Sigmoid" :
          let input$8 = Stdlib_Option.getOr(record.inputs[0], {
            nodeId: -1,
            bufferId: -1,
            shape: [],
            data: undefined
          });
          let inputSize$9 = Shape.numElements(input$8.shape);
          let gradInBufferId$9 = getOrAllocateGradient(engine, input$8.nodeId, inputSize$9);
          ops.contents = ops.contents.concat([{
              kernel: Autograd.genSigmoidBackwardKernel(outputSize),
              bindings: [
                [
                  "grad_out",
                  gradOutBufferId
                ],
                [
                  "out",
                  record.output.bufferId
                ],
                [
                  "grad_x",
                  gradInBufferId$9
                ]
              ],
              outputSize: outputSize
            }]);
          return;
        case "GeLU" :
          let input$9 = Stdlib_Option.getOr(record.inputs[0], {
            nodeId: -1,
            bufferId: -1,
            shape: [],
            data: undefined
          });
          let inputSize$10 = Shape.numElements(input$9.shape);
          let gradInBufferId$10 = getOrAllocateGradient(engine, input$9.nodeId, inputSize$10);
          ops.contents = ops.contents.concat([{
              kernel: Autograd.genGeLUBackwardKernel(outputSize),
              bindings: [
                [
                  "grad_out",
                  gradOutBufferId
                ],
                [
                  "x",
                  input$9.bufferId
                ],
                [
                  "grad_x",
                  gradInBufferId$10
                ]
              ],
              outputSize: outputSize
            }]);
          return;
        case "Add" :
          let input0 = Stdlib_Option.getOr(record.inputs[0], {
            nodeId: -1,
            bufferId: -1,
            shape: [],
            data: undefined
          });
          let input1 = Stdlib_Option.getOr(record.inputs[1], {
            nodeId: -1,
            bufferId: -1,
            shape: [],
            data: undefined
          });
          let size0 = Shape.numElements(input0.shape);
          let size1 = Shape.numElements(input1.shape);
          let gradA = getOrAllocateGradient(engine, input0.nodeId, size0);
          let gradB = getOrAllocateGradient(engine, input1.nodeId, size1);
          if (size0 === outputSize && size1 === outputSize) {
            ops.contents = ops.contents.concat([{
                kernel: Autograd.genAddBackwardKernel(outputSize),
                bindings: [
                  [
                    "grad_out",
                    gradOutBufferId
                  ],
                  [
                    "grad_a",
                    gradA
                  ],
                  [
                    "grad_b",
                    gradB
                  ]
                ],
                outputSize: outputSize
              }]);
          } else {
            ops.contents = ops.contents.concat([{
                kernel: Autograd.genAddBackwardKernel(outputSize),
                bindings: [
                  [
                    "grad_out",
                    gradOutBufferId
                  ],
                  [
                    "grad_a",
                    gradA
                  ],
                  [
                    "grad_b",
                    gradB
                  ]
                ],
                outputSize: outputSize
              }]);
          }
          return;
        case "Sub" :
          let input0$1 = Stdlib_Option.getOr(record.inputs[0], {
            nodeId: -1,
            bufferId: -1,
            shape: [],
            data: undefined
          });
          let input1$1 = Stdlib_Option.getOr(record.inputs[1], {
            nodeId: -1,
            bufferId: -1,
            shape: [],
            data: undefined
          });
          let size0$1 = Shape.numElements(input0$1.shape);
          let size1$1 = Shape.numElements(input1$1.shape);
          let gradA$1 = getOrAllocateGradient(engine, input0$1.nodeId, size0$1);
          let gradB$1 = getOrAllocateGradient(engine, input1$1.nodeId, size1$1);
          ops.contents = ops.contents.concat([{
              kernel: Autograd.genSubBackwardKernel(outputSize),
              bindings: [
                [
                  "grad_out",
                  gradOutBufferId
                ],
                [
                  "grad_a",
                  gradA$1
                ],
                [
                  "grad_b",
                  gradB$1
                ]
              ],
              outputSize: outputSize
            }]);
          return;
        case "Mul" :
          let input0$2 = Stdlib_Option.getOr(record.inputs[0], {
            nodeId: -1,
            bufferId: -1,
            shape: [],
            data: undefined
          });
          let input1$2 = Stdlib_Option.getOr(record.inputs[1], {
            nodeId: -1,
            bufferId: -1,
            shape: [],
            data: undefined
          });
          let size0$2 = Shape.numElements(input0$2.shape);
          let size1$2 = Shape.numElements(input1$2.shape);
          let gradA$2 = getOrAllocateGradient(engine, input0$2.nodeId, size0$2);
          let gradB$2 = getOrAllocateGradient(engine, input1$2.nodeId, size1$2);
          ops.contents = ops.contents.concat([{
              kernel: Autograd.genMulBackwardKernel(outputSize),
              bindings: [
                [
                  "grad_out",
                  gradOutBufferId
                ],
                [
                  "a",
                  input0$2.bufferId
                ],
                [
                  "b",
                  input1$2.bufferId
                ],
                [
                  "grad_a",
                  gradA$2
                ],
                [
                  "grad_b",
                  gradB$2
                ]
              ],
              outputSize: outputSize
            }]);
          return;
        case "Div" :
          let input0$3 = Stdlib_Option.getOr(record.inputs[0], {
            nodeId: -1,
            bufferId: -1,
            shape: [],
            data: undefined
          });
          let input1$3 = Stdlib_Option.getOr(record.inputs[1], {
            nodeId: -1,
            bufferId: -1,
            shape: [],
            data: undefined
          });
          let size0$3 = Shape.numElements(input0$3.shape);
          let size1$3 = Shape.numElements(input1$3.shape);
          let gradA$3 = getOrAllocateGradient(engine, input0$3.nodeId, size0$3);
          let gradB$3 = getOrAllocateGradient(engine, input1$3.nodeId, size1$3);
          ops.contents = ops.contents.concat([{
              kernel: Autograd.genDivBackwardKernel(outputSize),
              bindings: [
                [
                  "grad_out",
                  gradOutBufferId
                ],
                [
                  "a",
                  input0$3.bufferId
                ],
                [
                  "b",
                  input1$3.bufferId
                ],
                [
                  "grad_a",
                  gradA$3
                ],
                [
                  "grad_b",
                  gradB$3
                ]
              ],
              outputSize: outputSize
            }]);
          return;
        case "MatMul" :
          let input0$4 = Stdlib_Option.getOr(record.inputs[0], {
            nodeId: -1,
            bufferId: -1,
            shape: [],
            data: undefined
          });
          let input1$4 = Stdlib_Option.getOr(record.inputs[1], {
            nodeId: -1,
            bufferId: -1,
            shape: [],
            data: undefined
          });
          let shape0 = input0$4.shape;
          let shape1 = input1$4.shape;
          let r0 = shape0.length;
          let r1 = shape1.length;
          if (!(r0 >= 2 && r1 >= 2)) {
            return;
          }
          let m = Stdlib_Option.getOr(shape0[r0 - 2 | 0], 1);
          let k = Stdlib_Option.getOr(shape0[r0 - 1 | 0], 1);
          let n = Stdlib_Option.getOr(shape1[r1 - 1 | 0], 1);
          let size0$4 = Shape.numElements(shape0);
          let size1$4 = Shape.numElements(shape1);
          let gradA$4 = getOrAllocateGradient(engine, input0$4.nodeId, size0$4);
          let gradB$4 = getOrAllocateGradient(engine, input1$4.nodeId, size1$4);
          let batchDims = record.output.shape.slice(0, record.output.shape.length - 2 | 0);
          let batchSize = Stdlib_Array.reduce(batchDims, 1, (a, b) => a * b | 0);
          if (batchSize > 1) {
            ops.contents = ops.contents.concat([
              {
                kernel: Autograd.genBatchedMatMulBackwardAKernel(batchSize, m, k, n),
                bindings: [
                  [
                    "grad_out",
                    gradOutBufferId
                  ],
                  [
                    "b",
                    input1$4.bufferId
                  ],
                  [
                    "grad_a",
                    gradA$4
                  ]
                ],
                outputSize: size0$4
              },
              {
                kernel: Autograd.genBatchedMatMulBackwardBKernel(batchSize, m, k, n),
                bindings: [
                  [
                    "grad_out",
                    gradOutBufferId
                  ],
                  [
                    "a",
                    input0$4.bufferId
                  ],
                  [
                    "grad_b",
                    gradB$4
                  ]
                ],
                outputSize: size1$4
              }
            ]);
          } else {
            ops.contents = ops.contents.concat([
              {
                kernel: Autograd.genMatMulBackwardAKernel(m, k, n),
                bindings: [
                  [
                    "grad_out",
                    gradOutBufferId
                  ],
                  [
                    "b",
                    input1$4.bufferId
                  ],
                  [
                    "grad_a",
                    gradA$4
                  ]
                ],
                outputSize: size0$4
              },
              {
                kernel: Autograd.genMatMulBackwardBKernel(m, k, n),
                bindings: [
                  [
                    "grad_out",
                    gradOutBufferId
                  ],
                  [
                    "a",
                    input0$4.bufferId
                  ],
                  [
                    "grad_b",
                    gradB$4
                  ]
                ],
                outputSize: size1$4
              }
            ]);
          }
          return;
        default:
          return;
      }
    } else {
      switch (match.TAG) {
        case "LeakyReLU" :
          let input$10 = Stdlib_Option.getOr(record.inputs[0], {
            nodeId: -1,
            bufferId: -1,
            shape: [],
            data: undefined
          });
          let inputSize$11 = Shape.numElements(input$10.shape);
          let gradInBufferId$11 = getOrAllocateGradient(engine, input$10.nodeId, inputSize$11);
          ops.contents = ops.contents.concat([{
              kernel: Autograd.genLeakyReLUBackwardKernel(outputSize, match.alpha),
              bindings: [
                [
                  "grad_out",
                  gradOutBufferId
                ],
                [
                  "x",
                  input$10.bufferId
                ],
                [
                  "grad_x",
                  gradInBufferId$11
                ]
              ],
              outputSize: outputSize
            }]);
          return;
        case "Reduce" :
          switch (match.op) {
            case "Sum" :
              let input$11 = Stdlib_Option.getOr(record.inputs[0], {
                nodeId: -1,
                bufferId: -1,
                shape: [],
                data: undefined
              });
              let inputSize$12 = Shape.numElements(input$11.shape);
              let gradIn = getOrAllocateGradient(engine, input$11.nodeId, inputSize$12);
              ops.contents = ops.contents.concat([{
                  kernel: Autograd.genSumBackwardKernel(input$11.shape, record.output.shape, match.axes),
                  bindings: [
                    [
                      "grad_out",
                      gradOutBufferId
                    ],
                    [
                      "grad_x",
                      gradIn
                    ]
                  ],
                  outputSize: inputSize$12
                }]);
              return;
            case "Mean" :
              let input$12 = Stdlib_Option.getOr(record.inputs[0], {
                nodeId: -1,
                bufferId: -1,
                shape: [],
                data: undefined
              });
              let inputSize$13 = Shape.numElements(input$12.shape);
              let gradIn$1 = getOrAllocateGradient(engine, input$12.nodeId, inputSize$13);
              ops.contents = ops.contents.concat([{
                  kernel: Autograd.genMeanBackwardKernel(input$12.shape, record.output.shape, match.axes),
                  bindings: [
                    [
                      "grad_out",
                      gradOutBufferId
                    ],
                    [
                      "grad_x",
                      gradIn$1
                    ]
                  ],
                  outputSize: inputSize$13
                }]);
              return;
            default:
              return;
          }
        case "Reshape" :
        case "Squeeze" :
        case "Unsqueeze" :
        case "Flatten" :
        case "ExpandDims" :
          break;
        case "Softmax" :
          let axis = match.axis;
          let input$13 = Stdlib_Option.getOr(record.inputs[0], {
            nodeId: -1,
            bufferId: -1,
            shape: [],
            data: undefined
          });
          let inputShape = input$13.shape;
          let rank = inputShape.length;
          let normAxis = axis < 0 ? rank + axis | 0 : axis;
          let axisSize = Stdlib_Option.getOr(inputShape[normAxis], 1);
          let outerSize = Primitive_int.div(outputSize, axisSize);
          let inputSize$14 = Shape.numElements(input$13.shape);
          let gradIn$2 = getOrAllocateGradient(engine, input$13.nodeId, inputSize$14);
          ops.contents = ops.contents.concat([{
              kernel: Autograd.genSoftmaxBackwardKernel(outerSize, axisSize),
              bindings: [
                [
                  "grad_out",
                  gradOutBufferId
                ],
                [
                  "softmax_out",
                  record.output.bufferId
                ],
                [
                  "grad_x",
                  gradIn$2
                ]
              ],
              outputSize: inputSize$14
            }]);
          return;
        default:
          return;
      }
    }
    let input$14 = Stdlib_Option.getOr(record.inputs[0], {
      nodeId: -1,
      bufferId: -1,
      shape: [],
      data: undefined
    });
    let inputSize$15 = Shape.numElements(input$14.shape);
    let gradIn$3 = getOrAllocateGradient(engine, input$14.nodeId, inputSize$15);
    ops.contents = ops.contents.concat([{
        kernel: Autograd.genCopyBackwardKernel(outputSize),
        bindings: [
          [
            "grad_out",
            gradOutBufferId
          ],
          [
            "grad_x",
            gradIn$3
          ]
        ],
        outputSize: inputSize$15
      }]);
  });
  return ops.contents;
}

function getParameterGradients(engine) {
  return Stdlib_Array.filterMap(engine.parameters, param => {
    let gradBufferId = Belt_MapInt.get(engine.gradients, param.nodeId);
    if (gradBufferId !== undefined) {
      return [
        param.nodeId,
        gradBufferId,
        param.shape
      ];
    }
  });
}

function generateOptimizerOps(engine, optimizer, lr, beta1, beta2, epsilon, weightDecay) {
  engine.timestep = engine.timestep + 1 | 0;
  let t = engine.timestep;
  return Stdlib_Array.filterMap(engine.parameters, param => {
    let gradBufferId = Belt_MapInt.get(engine.gradients, param.nodeId);
    if (gradBufferId === undefined) {
      return;
    }
    let size = Shape.numElements(param.shape);
    switch (optimizer) {
      case "adam" :
        let state = Belt_MapInt.get(engine.adamStates, param.nodeId);
        let adamState;
        if (state !== undefined) {
          adamState = state;
        } else {
          let mId = (param.nodeId * 1000 | 0) + 600 | 0;
          let vId = (param.nodeId * 1000 | 0) + 700 | 0;
          let state$1 = {
            m: mId,
            v: vId
          };
          engine.adamStates = Belt_MapInt.set(engine.adamStates, param.nodeId, state$1);
          adamState = state$1;
        }
        return {
          kernel: Autograd.genAdamKernel(size, lr, beta1, beta2, epsilon, t),
          bindings: [
            [
              "param",
              param.bufferId
            ],
            [
              "grad",
              gradBufferId
            ],
            [
              "m",
              adamState.m
            ],
            [
              "v",
              adamState.v
            ]
          ],
          outputSize: size
        };
      case "adamw" :
        let state$2 = Belt_MapInt.get(engine.adamStates, param.nodeId);
        let adamState$1;
        if (state$2 !== undefined) {
          adamState$1 = state$2;
        } else {
          let mId$1 = (param.nodeId * 1000 | 0) + 600 | 0;
          let vId$1 = (param.nodeId * 1000 | 0) + 700 | 0;
          let state$3 = {
            m: mId$1,
            v: vId$1
          };
          engine.adamStates = Belt_MapInt.set(engine.adamStates, param.nodeId, state$3);
          adamState$1 = state$3;
        }
        return {
          kernel: Autograd.genAdamWKernel(size, lr, beta1, beta2, epsilon, weightDecay, t),
          bindings: [
            [
              "param",
              param.bufferId
            ],
            [
              "grad",
              gradBufferId
            ],
            [
              "m",
              adamState$1.m
            ],
            [
              "v",
              adamState$1.v
            ]
          ],
          outputSize: size
        };
      case "sgd" :
        return {
          kernel: Autograd.genSGDKernel(size, lr),
          bindings: [
            [
              "param",
              param.bufferId
            ],
            [
              "grad",
              gradBufferId
            ]
          ],
          outputSize: size
        };
      default:
        return;
    }
  });
}

function generateZeroGradOps(engine) {
  return Stdlib_Array.filterMap(engine.parameters, param => {
    let gradBufferId = Belt_MapInt.get(engine.gradients, param.nodeId);
    if (gradBufferId === undefined) {
      return;
    }
    let size = Shape.numElements(param.shape);
    return {
      kernel: Autograd.genGradZeroKernel(size),
      bindings: [[
          "grad",
          gradBufferId
        ]],
      outputSize: size
    };
  });
}

export {
  create,
  markRequiresGrad,
  needsGrad,
  record,
  clearTape,
  getOrAllocateGradient,
  generateBackwardOps,
  getParameterGradients,
  generateOptimizerOps,
  generateZeroGradOps,
}
/* Autograd Not a pure module */
